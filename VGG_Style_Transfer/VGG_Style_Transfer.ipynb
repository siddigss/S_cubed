{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "import requests\n",
    "from io import BytesIO\n",
    "import json\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting the Image.\n",
    "image_url = 'https://cdn.pixabay.com/photo/2017/09/13/15/38/airplane-2745898_960_720.jpg'\n",
    "#image_url = 'https://media.healthdirect.org.au/images/general/primary/baby-sleeping-AMCBWB.jpg'\n",
    "image = Image.open(BytesIO(requests.get(image_url).content))\n",
    "plt.axis('off')\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VGG19 and setting it to evaluation mode.\n",
    "vgg19 = torchvision.models.vgg19(pretrained=True).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting the labels\n",
    "!git clone https://github.com/anishathalye/imagenet-simple-labels.git\n",
    "\n",
    "labels_json = open('imagenet-simple-labels/imagenet-simple-labels.json')\n",
    "labels = json.load(labels_json)\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preparing the image\n",
    "input_size = 128 #....\n",
    "\n",
    "transform_to_input = torchvision.transforms.Compose([torchvision.transforms.Resize((input_size,input_size)),\n",
    "                                            torchvision.transforms.ToTensor()])\n",
    "\n",
    "\n",
    "input_image = transform_to_input(image).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The predictions.\n",
    "prediction = vgg19(input_image)\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Chcking the top 5 predictions\n",
    "prediction_copy = prediction.data.numpy().copy().squeeze(0)\n",
    "for _ in range(5):\n",
    "    index = prediction_copy.argmax()\n",
    "    print('class: {} with score: {}'.format(labels[index], prediction_copy[index]))\n",
    "    prediction_copy[index] = prediction_copy.min() - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We can reverse it !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we know the outputs of the convolutional neural network for some unkown image x.\n",
    "\n",
    "We can approximate x from the its neural network output by finding an image y that minimizes the error (for some error function) between the output of y and the output of x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers_list = [4] #The layers we are measuring the errors at. List because we can use more than one layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's prepare a new network, by inserting additional layers that measure the error after some convolutional layers.\n",
    "\n",
    "#The additional Layer\n",
    "class layer_error(torch.nn.Module):\n",
    "    def __init__(self, target):\n",
    "        super(layer_error, self).__init__()\n",
    "        self.target = target.detach()\n",
    "        self.error = torch.Tensor([0]).type(torch.FloatTensor)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        self.error = torch.nn.functional.mse_loss(input,self.target)\n",
    "        return input #To not mess the network\n",
    "    \n",
    "vgg19_features = copy.deepcopy(vgg19.features.eval())\n",
    "vgg19_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG_Normalization(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VGG_Normalization, self).__init__()\n",
    "        self.mean = torch.tensor([0.485, 0.456, 0.406]).view(-1, 1, 1)\n",
    "        self.std = torch.tensor([0.229, 0.224, 0.225]).view(-1, 1, 1)\n",
    "\n",
    "    def forward(self, img):\n",
    "        return (img - self.mean) / self.std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalization = VGG_Normalization()\n",
    "vgg_with_layer_errors = torch.nn.Sequential(normalization) #empty\n",
    "\n",
    "conv_layer_number=0\n",
    "layer_number = 0\n",
    "layer_errors = []\n",
    "\n",
    "for layer in vgg19_features.children():\n",
    "    \n",
    "    layer_number += 1\n",
    "    if not isinstance(layer, torch.nn.ReLU):\n",
    "        vgg_with_layer_errors.add_module('{}'.format(layer_number), layer)\n",
    "    else :\n",
    "        vgg_with_layer_errors.add_module('{}'.format(layer_number), torch.nn.ReLU(inplace=False))\n",
    "    \n",
    "    if isinstance(layer, torch.nn.Conv2d):\n",
    "        conv_layer_number += 1\n",
    "        if conv_layer_number in layers_list:\n",
    "            layer_number += 1\n",
    "            new_layer_error = layer_error(vgg_with_layer_errors(input_image))\n",
    "            vgg_with_layer_errors.add_module('{}'.format(layer_number), new_layer_error)\n",
    "            layer_errors.append(new_layer_error)\n",
    "    \n",
    "    if len(layer_errors) == len(layers_list):\n",
    "        break\n",
    "    \n",
    "vgg_with_layer_errors    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "approximate_image = torch.randn(input_image.data.size()) #Initial Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimizer = torch.optim.Adam([approximate_image.requires_grad_()], lr=0.01)\n",
    "optimizer = torch.optim.LBFGS([approximate_image.requires_grad_()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helping function to show images from tensors.\n",
    "\n",
    "image_extractor = torchvision.transforms.ToPILImage()\n",
    "\n",
    "def image_show(image_tensor):\n",
    "    image = image_tensor.clone().detach().squeeze(0)\n",
    "    image = image_extractor(image)\n",
    "    \n",
    "    plt.axis('off')\n",
    "    plt.imshow(image)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_show(approximate_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_show(input_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "approximate_images = [approximate_image.clone().detach()]\n",
    "number_of_iterations = 100\n",
    "\n",
    "begin = time.time()\n",
    "\n",
    "for step in range(number_of_iterations):   \n",
    "    \n",
    "    error = 0\n",
    "    def closure():\n",
    "        global error\n",
    "        error = 0\n",
    "        approximate_image.data.clamp_(0,1)\n",
    "        vgg_with_layer_errors(approximate_image)\n",
    "        error = torch.Tensor([0]).type(torch.FloatTensor)\n",
    "        for layer in layer_errors:\n",
    "            error += layer.error\n",
    "        optimizer.zero_grad()\n",
    "        error.backward()\n",
    "\n",
    "        return error\n",
    "    \n",
    "    optimizer.step(closure)\n",
    "    \n",
    "    if step% 10 == 0:\n",
    "        print('step: {} ,error = {:4f}'.format(step, error.item()))\n",
    "        image_show(approximate_image)\n",
    "        approximate_images.append(approximate_image.clone().detach())\n",
    "        \n",
    "end = time.time()\n",
    "print('Time elapsed: {}'.format(begin-end))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Style Transfer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define the vgg <b>style</b> of an image $X$ at layer $n$ to be the <b>Gram matrix</b> of the output of the layer $n$ once the image $X$ is feed into vgg.\n",
    "\n",
    "The <b>Gram matrix</b> $G$ of a sequence of vectors $v_1,...,v_k\\in\\mathbb{R}^d$ is defined as\n",
    "\n",
    "\n",
    "$$G=\\begin{pmatrix}\n",
    " \\left \\langle v_1,v_1 \\right \\rangle & \\left \\langle v_1,v_2 \\right \\rangle &  \\cdots & \\left \\langle v_1,v_k \\right \\rangle\\\\ \n",
    " \\left \\langle v_2,v_1 \\right \\rangle & \\left \\langle v_2,v_2 \\right \\rangle &  \\cdots & \\left \\langle v_2,v_k \\right \\rangle\\\\\n",
    " \\vdots & \\ddots&  &\\vdots\\\\\n",
    " \\vdots & & \\ddots &\\vdots\\\\\n",
    " \\left \\langle v_k,v_1 \\right \\rangle & \\left \\langle v_k,v_2 \\right \\rangle &  \\cdots & \\left \\langle v_k,v_k \\right \\rangle\\\\\n",
    "\\end{pmatrix}$$\n",
    "\n",
    "where $\\left \\langle v,u \\right \\rangle$ is \"the inner product\" between vectos $v$ and $u$ (Vectors here will be in fact matrices).\n",
    "\n",
    "Notice that if $A$ is the matrix whose rows are $v_1,...,v_k$ then $G=AA^{T}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A function that returns Gram matrix.\n",
    "def Gram_matrix(tensor_4d):\n",
    "    size = tensor_4d.size()\n",
    "    vectors_matrix = tensor_4d.view(size[1], size[2]*size[3]) #size[0]=1\n",
    "    G = torch.mm(vectors_matrix, vectors_matrix.t())\n",
    "    return G/(size[1]*size[2]*size[3]) #normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we saw in the previous section, we can reconstruct an image by knowing the vgg outputs at any convolutional layer.\n",
    "To transfer the <b>style</b> we want the reconstructed image to have a Gram matrix close to the Gram matrix of the image we want to the style from.\n",
    "\n",
    "Hence we will just adjust the loss layer we defined previously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The additional Layer adjusted for style transfer\n",
    "class layer_error_style(torch.nn.Module):\n",
    "    def __init__(self, content, style, style_weight = 10000): # as suggested in the paper, either 1000 or 10000.\n",
    "        super(layer_error_style, self).__init__()\n",
    "        self.content = content.detach()\n",
    "        self.style = style.detach()\n",
    "        self.error = torch.Tensor([0]).type(torch.FloatTensor)\n",
    "        self.content_error = torch.Tensor([0]).type(torch.FloatTensor)\n",
    "        self.style_error = torch.Tensor([0]).type(torch.FloatTensor)\n",
    "        self.style_weight = style_weight\n",
    "        \n",
    "    def forward(self, input):\n",
    "        self.content_error = torch.nn.functional.mse_loss(input,self.content)\n",
    "        self.style_error = self.style_weight*torch.nn.functional.mse_loss(Gram_matrix(input), Gram_matrix(self.style))\n",
    "        self.error = self.content_error + self.style_error\n",
    "        return input #To not mess the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We write the same things we wrote previously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Just some random images from google search :)\n",
    "content_url = 'https://upload.wikimedia.org/wikipedia/commons/d/d7/Leonhard_Euler.jpg'\n",
    "content = Image.open(BytesIO(requests.get(content_url).content))\n",
    "style_url = 'https://upload.wikimedia.org/wikipedia/en/8/8f/Pablo_Picasso%2C_1909-10%2C_Figure_dans_un_Fauteuil_%28Seated_Nude%2C_Femme_nue_assise%29%2C_oil_on_canvas%2C_92.1_x_73_cm%2C_Tate_Modern%2C_London.jpg'\n",
    "style = Image.open(BytesIO(requests.get(style_url).content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_image = transform_to_input(content).unsqueeze(0)\n",
    "style_image = transform_to_input(style).unsqueeze(0)\n",
    "#style_transferred_image = content_image.clone().detach() #Initial Value\n",
    "style_transferred_image = torch.randn(content_image.data.size())\n",
    "\n",
    "image_show(content_image)\n",
    "image_show(style_image)\n",
    "image_show(style_transferred_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers_list = [1,2,3,4,5] #The layers we are measuring the errors at. List because we can use more than one layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "normalization = VGG_Normalization()\n",
    "vgg_style_transfer = torch.nn.Sequential(normalization) #the new network with the first layer being normalization.\n",
    "\n",
    "layer_number = 0\n",
    "layer_errors = []\n",
    "conv_layer_number = 0\n",
    "\n",
    "for layer in vgg19_features.children():\n",
    "    \n",
    "    layer_number += 1    \n",
    "    if not isinstance(layer, torch.nn.ReLU):\n",
    "        vgg_style_transfer.add_module('{}'.format(layer_number), layer)\n",
    "    else :\n",
    "        vgg_style_transfer.add_module('{}'.format(layer_number), torch.nn.ReLU(inplace=False)) #error.Backward() complains wthen inplace=True.\n",
    "\n",
    "    if isinstance(layer, torch.nn.Conv2d):\n",
    "        conv_layer_number +=1\n",
    "        if conv_layer_number in layers_list:\n",
    "            layer_number+=1\n",
    "            new_layer_error = layer_error_style(vgg_style_transfer(content_image), vgg_style_transfer(style_image)\n",
    "                                                ,style_weight=100000) #Try different weights.\n",
    "            vgg_style_transfer.add_module('{}'.format(layer_number), new_layer_error)\n",
    "            layer_errors.append(new_layer_error)\n",
    "            \n",
    "    if len(layer_errors) == len(layers_list):\n",
    "        break\n",
    "    \n",
    "vgg_style_transfer    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#optimizer = torch.optim.Adam([style_transferred_image.requires_grad_()], lr=0.1)\n",
    "optimizer = torch.optim.LBFGS([style_transferred_image.requires_grad_()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "style_transferred_images = [style_transferred_image.clone().detach()]\n",
    "number_of_iterations = 50\n",
    "\n",
    "begin = time.time()\n",
    "for step in range(number_of_iterations):   \n",
    "    \n",
    "    error = torch.Tensor([0.0])\n",
    "    content_error = torch.Tensor([0.0])\n",
    "    style_error = torch.Tensor([0.0])\n",
    "    def closure():\n",
    "        global error\n",
    "        global content_error\n",
    "        global style_error\n",
    "        error = torch.Tensor([0.0])\n",
    "        content_error = torch.Tensor([0.0])\n",
    "        style_error = torch.Tensor([0.0])\n",
    "        style_transferred_image.data.clamp_(0,1)\n",
    "        vgg_style_transfer(style_transferred_image)\n",
    "\n",
    "        for layer in layer_errors:\n",
    "            error += layer.error\n",
    "            content_error += layer.content_error\n",
    "            style_error += layer.style_error\n",
    "        optimizer.zero_grad()\n",
    "        error.backward()\n",
    "\n",
    "        return error\n",
    "    \n",
    "    optimizer.step(closure)\n",
    "    \n",
    "    if step% 1 == 0:\n",
    "        print('step: {} , error = {:4f} , style error = {:4f} , content error = {:4f}'.format(step, error.item(), \n",
    "                                                                         content_error.item(), style_error.item()))\n",
    "        image_show(style_transferred_image)\n",
    "        style_transferred_images.append(style_transferred_image.clone().detach())\n",
    "\n",
    "end = time.time()\n",
    "print('Time elapsed: {}'.format(end-begin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(style_transferred_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_image = image_extractor(style_transferred_images[100].squeeze(0))\n",
    "\n",
    "plt.imshow(final_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's try to smoothen the image.\n",
    "from skimage.restoration import (denoise_tv_chambolle, denoise_bilateral,\n",
    "                                 denoise_wavelet, estimate_sigma)\n",
    "from skimage import data, img_as_float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_image_float = img_as_float(final_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    plt.imshow(denoise_tv_chambolle(final_image_float, weight= (i+1)/100, multichannel=True))\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
